{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%; height:60px; background-color:aqua; display:flex\">\n",
    "<div style=\"display:flex; flex-direction:row; justify-content:center\">\n",
    "<h2 style=\"color:white\"><strong>Support Vector Machine(SVM)</strong></h2>\n",
    "</div>\n",
    "</div>\n",
    "<h3 style=\"color:white\">Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.</h3>\n",
    "<h4 style=\"color:grey; font-style:italic\">The advantages of support vector machines are:\n",
    "<ul style=\"color:aqua; font-style:italic\">\n",
    "<li>\n",
    "Effective in high dimensional spaces.\n",
    "</li>\n",
    "<li>\n",
    "Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "</li>\n",
    "<li>\n",
    "Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "</li>\n",
    "<li>\n",
    "Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "</li>\n",
    "</ul>\n",
    "</h4>\n",
    "<h4 style=\"color:grey; font-style:italic\">The disadvantages of support vector machines include:\n",
    "<ul style=\"color:aqua; font-style:italic\">\n",
    "<li>\n",
    "If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "</li>\n",
    "<li>\n",
    "SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
    "</li>\n",
    "</ul>\n",
    "</h4>\n",
    "<h4 style=\"color:white\">Generally, Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.</h4>\n",
    "<div style=\"width:100%; height:auto; display:flex; flex-direction:row; justify-content:space-around;\">\n",
    "<img style=\"width:40%;height:auto; background-color:white\" src=\"/home/ahmedunix/data_Science_Work/Machine_Learning/11_Support_Vector_Machine/support-vector-machine-algorithm.png\">\n",
    "<img style=\"width:55%;height:auto\" src=\"/home/ahmedunix/data_Science_Work/Machine_Learning/11_Support_Vector_Machine/hyperplane.png\">\n",
    "</div>\n",
    "<h2 style=\"color:white; font-weight:bold\">How does SVM work?</h2>\n",
    "<h4 style=\"color:white; font-style:italic\">The main objective is to segregate the given dataset in the best possible way. The distance between the either nearest points is known as the margin. The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. SVM searches for the maximum marginal hyperplane in the following steps:\n",
    "<ol style=\"color:aqua; font-style:italic\">\n",
    "<li>\n",
    "Generate hyperplanes which segregates the classes in the best way. Left-hand side figure showing three hyperplanes black, blue and orange. Here, the blue and orange have higher classification error, but the black is separating the two classes correctly.\n",
    "</li>\n",
    "<li>\n",
    "Select the right hyperplane with the maximum segregation from the either nearest data points as shown in the right-hand side figure.\n",
    "</li>\n",
    "</li>\n",
    "</ol>\n",
    "</h4>\n",
    "<img style=\"width:50%; height:auto\"  src=\"/home/ahmedunix/data_Science_Work/Machine_Learning/11_Support_Vector_Machine/svm_working.webp\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import tensorflow as ts, keras\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,LabelEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-work-mqn9WFvk-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
